version: "3.4"

services:
  aggregator:
    image: zlynch/clickstream:1.0
    hostname: clickstream-aggregator
    networks:
      - zk-net
    environment:
      PYTHONPATH: /opt/woody
      # outside cluster: 192.168.70.220,192.168.70.189,192.168.70.35
      #                  17001
      CASSANDRA_SERVERS: cass1,cass2,cass3
      CASSANDRA_PORT: 9042
      CASSANDRA_USER: dev
      CASSANDRA_PASS: dcb5c69fedb78e9a29a3
      # outside cluster: 192.168.70.220:17812,192.168.70.35:17812,192.168.70.94:17812
      KAFKA_BROKERS: k1:17812,k2:17812,k3:17812
      SPARK_HOME: /opt/spark
      #SPARK_MASTER: spark://spark1:7077
      SPARK_SCHED_MODE: FAIR
      SPARK_SCHED_POOL: clickstream
      SPARK_SCHED_FILE: /opt/spark/conf/fairscheduler.xml
      SPARK_LOCAL_DIRS: /data
      SPARK_EXTRA_CLASSPATH: /opt/spark-extern
      PYSPARK_MAJOR_PYTHON_VERSION: 3
#    ports:
#      - published: 18881
#        target: 4040
#        mode: host
        #command: /opt/spark/bin/spark-submit --master spark://spark1:7077 --jars /opt/spark-extern/spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar woody/apps/clickstream.py
#    volumes:
#       - /fedev-data/spark-conf:/data/spark-conf
#       - /fedev-data:/data
#      - /etc/kubernetes:/etc/kubernetes
#      - /spark-cluster:/var/run/secrets/kubernetes.io/serviceaccount
    #--master k8s://https://192.168.70.140:6443
    #--master spark://spark1:7077
    #--conf spark.kubernetes.container.image=zlynch/spark:2.4.0
    #--conf spark.kubernetes.container.image=kubespark/spark-driver:v2.2.0-kubernetes-0.5.0
    #--conf spark.kubernetes.authenticate.submission.oauthToken=spark-token-c4kpx
    #--conf spark.kubernetes.authenticate.driver.oauthToken=spark-token-c4kpx
    #--conf spark.kubernetes.driver.container.image=kubespark/spark-driver-py:v2.2.0-kubernetes-0.5.0
    #--conf spark.kubernetes.executor.container.image=kubespark/spark-executor-py:v2.2.0-kubernetes-0.5.0
    #--conf spark.kubernetes.pyspark.pythonVersion=2
    #--conf spark.kubernetes.driver.pod.name=spark-drv-clickstream
    #--conf spark.kubernetes.driver.secrets.spark-secret=spark-conf-secrets
    #--conf spark.kubernetes.executor.secrets.spark-secret=spark-conf-secrets
#    --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-data.mount.path=spark-secret
#    --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-data.mount.readOnly=false
#    --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-data.options.claimName=spark-data
#    --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-data.mount.path=spark-secret
#    --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-data.options.claimName=spark-data
#    --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-data.mount.readOnly=false
#    --conf spark.kubernetes.driver.container.image=kubespark/spark-driver-py:v2.2.0-kubernetes-0.5.0
#    --conf spark.kubernetes.executor.container.image=kubespark/spark-executor-py:v2.2.0-kubernetes-0.5.0
#    local:///opt/woody/woody/apps/clickstream.py"
#    --packages com.datastax.spark:spark-cassandra-connector_2.11:2.4.0
#    --jars /opt/spark-extern/spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar 
#    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark
    command: "/opt/spark/bin/spark-submit 
    --name clickstream-aggregator
    --master k8s://http://192.168.70.140:8080
    --deploy-mode cluster
    --conf spark.app.name=clickstream-aggregator
    --conf spark.executor.instances=2
    --conf spark.kubernetes.container.image=zlynch/clickstream:1.0
    --conf spark.kubernetes.submission.waitAppCompletion=true
    --conf spark.kubernetes.container.image.pullPolicy=Always
    --conf spark.kubernetes.driverEnv.SPARK_EXTRA_CLASSPATH=/opt/spark-extern
    --conf spark.kubernetes.driverEnv.SPARK_MASTER=k8s://http://192.168.70.140:8080
    --conf spark.scheduler.mode=FAIR
    --conf spark.cassandra.connection.host=cass1,cass2,cass3
    --conf spark.cassandra.connection.port=9042
    --conf spark.cassandra.input.consistency.level=ONE
    --conf spark.cassandra.output.consistency.level=ONE
    --jars /opt/spark-extern/spark-*jar
    woody/apps/clickstream.py"
    deploy:
      placement:
        constraints: [node.labels.kfknode == 2]
  producer:
    image: zlynch/clickstream-producer:1.0
    hostname: clickstream-producer
    networks:
      - zk-net
    # outside cluster: 192.168.70.220:17812,192.168.70.35:17812,192.168.70.94:17812
    volumes:
      - /fedev-data/yoochoose:/data/yoochoose
      - /tmp:/tmp
    command: build/click_producer -alsologtostderr -brokers k1:17812,k2:17812,k3:17812
    deploy:
      placement:
        constraints: [node.labels.kfknode == 2]
      replicas: 1

networks:
  zk-net:
    external:
      name: kfk_zk-net
