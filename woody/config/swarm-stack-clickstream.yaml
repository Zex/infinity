version: "3.4"

services:
  aggregator:
    image: zlynch/clickstream:1.0
    hostname: clickstream-aggregator
    networks:
      - zk-net
    environment:
      PYTHONPATH: /opt/woody
      # outside cluster: 192.168.70.220,192.168.70.189,192.168.70.35
      #                  17001
      TZ: Asia/Hong_Kong
      CASSANDRA_SERVERS: cass1,cass2,cass3
      CASSANDRA_PORT: 9042
      CASSANDRA_USER: dev
      CASSANDRA_PASS: dcb5c69fedb78e9a29a3
      # outside cluster: 192.168.70.220:17812,192.168.70.35:17812,192.168.70.94:17812
      KAFKA_BROKERS: k1:17812,k2:17812,k3:17812
      SPARK_HOME: /opt/spark
      #SPARK_MASTER: spark://spark1:7077
      SPARK_SCHED_MODE: FAIR
      SPARK_SCHED_POOL: clickstream
      SPARK_SCHED_FILE: /opt/spark/conf/fairscheduler.xml
      SPARK_LOCAL_DIRS: /data
      SPARK_EXTRA_CLASSPATH: /opt/spark-extern
      PYSPARK_MAJOR_PYTHON_VERSION: 3
#    ports:
#      - published: 18881
#        target: 4040
#        mode: host
#    --conf spark.kubernetes.container.image=zlynch/spark:2.4.0
#    --conf spark.kubernetes.container.image=kubespark/spark-driver:v2.2.0-kubernetes-0.5.0
#    --conf spark.kubernetes.authenticate.submission.oauthToken=spark-token-c4kpx
#    --conf spark.kubernetes.authenticate.driver.oauthToken=spark-token-c4kpx
#    --conf spark.kubernetes.driver.container.image=kubespark/spark-driver-py:v2.2.0-kubernetes-0.5.0
#    --conf spark.kubernetes.executor.container.image=kubespark/spark-executor-py:v2.2.0-kubernetes-0.5.0
#    --conf spark.kubernetes.pyspark.pythonVersion=2
#    --conf spark.kubernetes.driver.pod.name=spark-drv-clickstream
#    --conf spark.kubernetes.driver.secrets.spark-secret=spark-conf-secrets
#    --conf spark.kubernetes.executor.secrets.spark-secret=spark-conf-secrets
#    --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-data.mount.path=spark-secret
#    --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-data.mount.readOnly=false
#    --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-data.options.claimName=spark-data
#    --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-data.mount.path=spark-secret
#    --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-data.options.claimName=spark-data
#    --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-data.mount.readOnly=false
#    --conf spark.kubernetes.driver.container.image=kubespark/spark-driver-py:v2.2.0-kubernetes-0.5.0
#    --conf spark.kubernetes.executor.container.image=kubespark/spark-executor-py:v2.2.0-kubernetes-0.5.0
#    local:///opt/woody/woody/apps/clickstream.py"
#    --packages com.datastax.spark:spark-cassandra-connector_2.11:2.4.0
#    --jars /opt/spark-extern/spark-streaming-kafka-0-8-assembly_2.11-2.4.0.jar 
#    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark
#    --master k8s://https://192.168.70.140:6443
#    --deploy-mode: cluster
#    --master spark://spark1:7077
#    --deploy-mode: client
#    --conf spark.scheduler.mode=FAIR
#    --deploy-mode cluster
#    --master spark://spark1:7077
#    --conf spark.kubernetes.authenticate.driver.clientKeyFile=/opt/woody/config/kube/pki/front-proxy-client.key
#    --conf spark.kubernetes.authenticate.driver.clientCertFile=/opt/woody/config/kube/pki/front-proxy-client.crt
#    --conf spark.kubernetes.driverEnv.SPARK_MASTER_IP=10.244.0.100
    command: "/opt/spark/bin/spark-submit 
    --name clickstream-aggregator
    --master k8s://https://192.168.70.140:6443
    --deploy-mode cluster
    --conf spark.scheduler.mode=FAIR
    --conf spark.app.name=clickstream-aggregator
    --conf spark.executor.instances=2
    --conf spark.kubernetes.authenticate.submission.caCertFile=/opt/woody/config/kube/pki/ca.crt
    --conf spark.kubernetes.authenticate.driver.caCertFile=/opt/woody/config/kube/pki/ca.crt
    --conf spark.kubernetes.driver.pod.name=spark-drv-clickstream
    --conf spark.kubernetes.authenticate.submission.clientCertFile=/opt/woody/config/kube/pki/apiserver-kubelet-client.crt
    --conf spark.kubernetes.authenticate.submission.clientKeyFile=/opt/woody/config/kube/pki/apiserver-kubelet-client.key
    --conf spark.kubernetes.authenticate.driver.clientKeyFile=/opt/woody/config/kube/pki/apiserver-kubelet-client.key
    --conf spark.kubernetes.authenticate.driver.clientCertFile=/opt/woody/config/kube/pki/apiserver-kubelet-client.crt
    --conf spark.kubernetes.namespace=default
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark
    --conf spark.kubernetes.submission.waitAppCompletion=true
    --conf spark.kubernetes.container.image=zlynch/clickstream:1.0
    --conf spark.kubernetes.container.image.pullPolicy=Always
    --conf spark.kubernetes.driverEnv.SPARK_EXTRA_CLASSPATH=/opt/spark-extern
    --conf spark.kubernetes.driverEnv.SPARK_MASTER=k8s://https://192.168.70.140:6443
    --conf spark.kubernetes.driverEnv.SPARK_LOCAL_IP=localhost
    --conf spark.cassandra.connection.host=cass1
    --conf spark.cassandra.connection.port=9042
    --conf spark.cassandra.input.consistency.level=ONE
    --conf spark.cassandra.output.consistency.level=ONE
    --jars /opt/spark-extern/spark-*jar
    woody/apps/clickstream.py"
    deploy:
      placement:
        constraints: [node.labels.kfknode == 2]
  producer:
    image: zlynch/woody-bg:1.0
    hostname: clickstream-producer
    networks:
      - zk-net
    # outside cluster: 192.168.70.220:17812,192.168.70.35:17812,192.168.70.94:17812
    environment:
      TZ: Asia/Hong_Kong
    volumes:
      - /fedev-data/yoochoose:/data/yoochoose
    command: build/click_producer -brokers k1:17812,k2:17812,k3:17812
    deploy:
      placement:
        constraints: [node.labels.sparknode == 1]
      replicas: 1
  apiserver:
    image: zlynch/woody-bg:1.0
    hostname: apiserver
    networks:
      - zk-net
    ports:
      - published: 17483
        target: 17483
        mode: host
    environment:
      TZ: Asia/Hong_Kong
    command: "build/apiserver
    -cass cass1,cass2,cass3
    -cass_port 9042 
    -apiaddr :17483
    -origins http://192.168.70.203:*,http://apiserver:*
    -cass_user dev
    -cass_pass dcb5c69fedb78e9a29a3"
    deploy:
      replicas: 1
      placement:
        constraints: [node.labels.sparknode == 1]
  ui:
    image: zlynch/woody-ui:1.0
    hostname: ui
    depends_on:
      - apiserver
    networks:
      - zk-net
    environment:
      TZ: Asia/Hong_Kong
    ports:
      - published: 17484
        target: 80
        mode: host
    deploy:
      placement:
        constraints: [node.labels.sparknode == 1]
      replicas: 1

networks:
  zk-net:
    external:
      name: kfk_zk-net
